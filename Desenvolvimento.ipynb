{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "#imporst deep leraning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Activation\n",
    "\n",
    "#import pandas profiling\n",
    "import random \n",
    "import names\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "train_data = train_data.sample(frac = 1) # Randomly Smaple data, ratio is 100%\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [[\"id\",\"Qualitative Nominal\"],[\"title\",\"Qualitative Nominal\"],\n",
    "         [\"author\",\"Qualitative Nominal\"],[\"text\",\"Qualitative Nominal\"],\n",
    "         [\"label\",\"Discrete Quantitative\"]]\n",
    "\n",
    "filing = pd.DataFrame(table, columns=[\"Variable\", \"Classification\"])\n",
    "filing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionaty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **fake news** file contains actual information about ... as follows:\n",
    "\n",
    "\n",
    "**train.csv:** A full training dataset with the following attributes:\n",
    "\n",
    "\n",
    "- **ID:** unique id for a news article\n",
    "\n",
    "\n",
    "- **TITLE:** the title of a news article\n",
    "\n",
    "\n",
    "- **AUTHOR:** author of the news article\n",
    "\n",
    "\n",
    "- **TEXT:** describe....\n",
    "\n",
    "\n",
    "- **LABEL:** a label that marks the article as potentially unreliable\n",
    " - 1: unreliable\n",
    " - 0: reliable\n",
    "\n",
    "\n",
    "**test.csv:** A testing training dataset with all the same attributes at train.csv without the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When importing the data we need to understand them and identify the range of specific predictors, identify the data type of each predictor, as well as calculate the number or percentage of missing values ​​for each predictor. We will use the pandas_profiling library which provides many extremely useful functions for exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pandas_profiling.ProfileReport(train_data)\n",
    "display(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to know which columns are missing data and in what proportion. Lack of data can affect training and lead to learning failures. So, is it possible to tell if there is missing data in the dataset? Yes, by the report generated by pandas_profiting, we have identified:\n",
    "\n",
    "Attribute title has 558 samples ( 2.68%) with missing values.\n",
    "Attribute author has 1957 samples ( 9.41%) with missing values.\n",
    "Attribute text has 39 samples ( 0.19%) with missing values.\n",
    "\n",
    "We have decided to eliminate the rows that there is any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before dropna we have {} lines in train'.format(train_data.shape[0]))\n",
    "train_data.dropna(inplace=True)\n",
    "print('After dropna we have {} lines in train'.format(train_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After eliminating the missing data lines, we applied a descriptive analysis to check if there was an imbalance in the data, and we identified that 7% of the data is unbalanced. In this step we will balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.label.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unreliable = train_data[train_data['label'] == 1]\n",
    "print('Unreliable：', len(unreliable))\n",
    "\n",
    "reliable = train_data[train_data['label'] == 0]\n",
    "print('Reliable：', len(reliable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply random shuffle to balance the number of untrusted and trusted records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = min(len(unreliable), len(reliable))\n",
    "\n",
    "un_data = unreliable.sample(n = mean)\n",
    "print('Unreliable：', len(un_data))\n",
    "r_data = reliable.sample(n = mean)\n",
    "print('Reliable：', len(r_data))\n",
    "\n",
    "train_data = pd.concat([un_data, r_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next profiling step, we created two new features, named: title_author_text and len_title_author_text.\n",
    "These columns store the concatenation of the title, author, text, and the total sentence size.\n",
    "Training will be based on this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['title_author_text'] = train_data['title'] + ' ' + train_data['author'] + ' ' + train_data['text']\n",
    "train_data['len_title_author_text'] = [len(x) for x in train_data['title_author_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail = train_data['len_title_author_text'].describe()\n",
    "print(detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for training the model will be separated by 80% for training and 20% for tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_features = train_data.drop(['id', 'label'], axis=1)\n",
    "train_features = train_data['title_author_text']\n",
    "train_targets = train_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_features, train_targets, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Train Data Feature: {}'.format(len(X_train)))\n",
    "print('Train Data Label: {}'.format(len(y_train)))\n",
    "\n",
    "print('Test Data Feature: {}'.format(len(X_test)))\n",
    "print('Test Data Label: {}'.format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "# This method is called when RandomState is initialized.\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use a token dictionary with no more than 5000 words, a reasonable token number for the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token = 5000\n",
    "token = Tokenizer(num_words = num_token, filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "token.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We truncate and fill the input sequences so they are the same size for modeling, since vectors of the same length are required to perform the calculation in Keras.\n",
    "The maximum length of the feature string will be the feature's average (len_title_author_text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = int(detail['mean'])\n",
    "\n",
    "X_train_token = token.texts_to_sequences(X_train)\n",
    "X_test_token = token.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(X_train_token, maxlen=max_review_length)\n",
    "X_test_seq = sequence.pad_sequences(X_test_token, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In creating the model, the first layer is the Embedded Layer that uses 32 length vectors to represent each word. The next layer is the LSTM layer with 100 units of memory. Since this is a classification problem, we use a Densa output layer with a single neuron and a sigmoid activation function to make predictions 0 or 1 for both classes (Unreliable and Reliable).\n",
    "\n",
    "Recurrent neural networks such as LSTM generally have the overfitting problem, so we apply the elimination layers with Dropout Keras.\n",
    "\n",
    "Since this work is a classification problem, it is important to identify the logloss value, and for this we apply the function (binary_crossentropy) and the ADAM optimization algorithm. We only added two times with a batch of 64 ratings to space the weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 32\n",
    "lstm_dim = 100\n",
    "dropout = 0.2\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=num_token, output_dim=embedding_vector_length, input_length=max_review_length))\n",
    "# model.add(Bidirectional(LSTM(lstm_dim), merge_mode = 'sum'))\n",
    "# model.add(Dense(units = 256, activation = 'relu'))\n",
    "# model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "# model.add(Dropout(dropout))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_token, output_dim=embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(lstm_dim))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(Dropout(dropout))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train_seq, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test_seq, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_data['id']\n",
    "X_test_data = test_data['title'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_token = token.texts_to_sequences(X_test_data)\n",
    "X_test_seq = sequence.pad_sequences(X_test_token, maxlen = max_review_length)\n",
    "\n",
    "predict = model.predict_classes(X_test_seq)\n",
    "predict_classes = predict.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = [predict for predict in predict_classes]\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_data[['id', 'label']]\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any results you write to the current directory are saved as output.\n",
    "result.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
