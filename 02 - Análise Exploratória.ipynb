{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "</p><h1 style=\"text-align: center;\"><strong>Detec&ccedil;&atilde;o automatizada de Fake News</strong></h1>\n",
    "<h1 style=\"text-align: center;\"><strong>e o problema da linguagem ofensiva</strong></h1>\n",
    "<h1 style=\"text-align: center;\"><strong>e &oacute;dio</strong></h1>\n",
    "<p>&nbsp;</p><p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</p><h4 style=\"text-align: center;\"><strong>Proposta de Tema para Trabalho de Conclusão do Curso de Especialização em Ciência de Dados.</strong></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</p><h3 style=\"text-align: center;\"><strong>Faculdade de Engenharia de Sorocaba</strong></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposto por:\n",
    "\n",
    "Alex Cozer Abrantes RA:183150\n",
    "\n",
    "Bruno Alves Comitre RA:183141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><a href='#analise_exploratoria'>1. Análise Explorátoria</a></li>\n",
    "    <ul>\n",
    "        <li><a href='#imports'>1.1 Imports</a></li>\n",
    "        <li><a href='#leitura_dataset'>1.2 Leitura do Dataset</a></li>\n",
    "        <li><a href='#classificacao_features'>1.3 Classificação das Features</a></li>\n",
    "        <li><a href='#dicionario_dados'>1.4 Dicionário dos Dados</a></li>\n",
    "        <li><a href='#analise_dados'>1.5 Análise dos Dados</a></li>\n",
    "        <li><a href='#dados_faltantes'>1.6 Dados Faltantes</a></li>\n",
    "        <li><a href='#dados_desiquilibrados'>1.7 Dados Desiquilibrados e Random Shuffle</a></li>\n",
    "        <li><a href='#novas_features'>1.8 Novos Recursos (Features)</a></li>\n",
    "        <ul>\n",
    "            <li><a href='#recursos_palavras'>Recursos por Palavras (Tokens) e Entidades</a></li>               \n",
    "        </ul>\n",
    "        <li><a href='#analise_grafica'>1.9 Análise Gráfica</a></li>\n",
    "        <ul>\n",
    "            <li><a href='#wordcloud'>1.9.1 WordCloud</a></li>\n",
    "            <li><a href='#histograma'>1.9.2 Histograma</a></li>\n",
    "            <ul>\n",
    "                <li><a href='#histo_titulo'>1.9.2.1 Histograma dos Títulos</a></li>\n",
    "                <li><a href='#histo_texto'>1.9.2.2 Histograma dos Textos</a></li>\n",
    "            </ul>\n",
    "            <li><a href='#boxplot'>1.9.3 Diagrama de Caixa</a></li>\n",
    "            <ul>\n",
    "                <li><a href='#boxplot_titulo'>1.9.3.1 Diagrama de Caixa dos Títulos</a></li>\n",
    "                <li><a href='#boxplot_texto'>1.9.3.2 Diagrama de Caixa dos Textos</a></li>\n",
    "            </ul>\n",
    "            <li><a href='#matriz_correlacao'>1.9.5 Matriz de correlação</a></li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analise_exploratoria'></a>\n",
    "# 1. Análise Explorátoria:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta fase do trabalho, inicialmente, faz necessário a aplicação de técnicas para manusear valores faltantes e fazer transformações de variáveis. Os dados serão ajustados e estreitando os presupostos para empregar técnicas gráficas e quantitativas, visando maximizar a obtenção de informações, tendências e detectação decomportamentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "## 1.1 Definições iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random \n",
    "import pandas_profiling #conda install -c conda-forge pandas-profiling\n",
    "import names #pip install names\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "import string\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from pandas.io.json import json_normalize\n",
    "from plotly import tools\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='leitura_dataset'></a>\n",
    "## 1.2 Leitura do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "## arquivo disponivel em: https://drive.google.com/drive/folders/1LqNzxY8l0EgznlCD-g873VD-ys1BRN2-?fbclid=IwAR3PPLu4hgNdKfQDJzrLGwV6L42Vm3xBrrcquuCOR4ySS97bVvU46JDaR2s\n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "train_data = train_data.sample(frac = 1) # Randomly Smaple data, ratio is 100%\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classificacao_features'></a>\n",
    "## 1.3 Classificação das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [[\"id\",\"Nominal Qualitativo\"],[\"title\",\"Nominal Qualitativo\"],\n",
    "         [\"author\",\"Nominal Qualitativo\"],[\"text\",\"Nominal Qualitativo\"],\n",
    "         [\"label\",\"Quantitativo Discreto\"]]\n",
    "\n",
    "filing = pd.DataFrame(table, columns=[\"Variável\", \"Classificação\"])\n",
    "filing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dicionario_dados'></a>\n",
    "## 1.4 Dicionário dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Dataset \"fake news\" contém as seguintes informações:\n",
    "\n",
    "\n",
    "- **ID:** id único da notícia\n",
    "\n",
    "\n",
    "- **TITLE:** título da notícia\n",
    "\n",
    "\n",
    "- **AUTHOR:** autor da notícia\n",
    "\n",
    "\n",
    "- **TEXT:** texto da notícia\n",
    "\n",
    "\n",
    "- **LABEL:** rótulo que marca se a notícia é potencialmente não confiável\n",
    " - 1: não confiável\n",
    " - 0: confiável"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analise_dados'></a>\n",
    "## 1.5 Análise dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao importar os dados, é importante entender e identificar o intervalo de preditores específicos, identificar o tipo de dados de cada preditor, bem como calcular o número ou a porcentagem de valores omissos para cada preditor. Usaremos a biblioteca pandas_profiling, que fornece muitas funções extremamente úteis para a análise exploratória de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observação: Foi retitado para demonstração a aleatoriedade dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_profiling = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pandas_profiling.ProfileReport(train_data_profiling)\n",
    "display(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dados_faltantes'></a>\n",
    "## 1.6 Dados Faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A falta de dados pode afetar a análise e o treinamento, que poderá levar a falhas no aprendizado. Então, é possível dizer se há dados ausentes no conjunto de dados? Sim, pelo relatório gerado por pandas_profiting, identificou-se:\n",
    "\n",
    "O título do atributo tem 558 amostras (2,68%) com valores ausentes.\n",
    "O autor do atributo possui 1957 amostras (9,41%) com valores ausentes.\n",
    "O texto do atributo tem 39 amostras (0,19%) com valores ausentes.\n",
    "\n",
    "Como existe dados faltantes nas 3 features do dataset (title, Author e Text) eliminar as linhas em que há dados ausentes neste caso é a melhor opção para não comprometer a análise e o treimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Antes do dropna tínhamos {} registros'.format(train_data.shape[0]))\n",
    "train_data.dropna(inplace=True)\n",
    "print('Depois do dropna temos {} registro'.format(train_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dados_desiquilibrados'></a>\n",
    "## 1.7 Dados Desiquilibrados e Random Shuflle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após eliminar as linhas de dados faltantes, aplicou-se uma análise descritiva para verificar se houve desequilíbrio nos dados, constatou-se que a média de 43% da feature Label, na qual armazena valores de 0 ou 1 (não confiáveis e confiáveis) mostra que os dados estão em desequilibrio, constando uma diferença de 2437 registros com o valor 1 (confiáveis) a mais que o valor 0 (não confiáveis). Para manter o conjunto de dados equilibrado aplicou-se o Random Shuffle nos registros com valor 1 (confiáveis) considerando o valor de 7924 registros que é o número total de dados (não confiáveis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.label.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unreliable = train_data[train_data['label'] == 1]\n",
    "print('Não confiável：', len(unreliable))\n",
    "\n",
    "reliable = train_data[train_data['label'] == 0]\n",
    "print('Confiável：', len(reliable))\n",
    "\n",
    "print('Desequilibrio nos dados de {} registros confiáveis'.format(len(reliable) - len(unreliable)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('aplicando Random Shuffle')\n",
    "mean = min(len(unreliable), len(reliable))\n",
    "\n",
    "un_data = unreliable.sample(n = mean)\n",
    "print('Não confiável：', len(un_data))\n",
    "r_data = reliable.sample(n = mean)\n",
    "print('Confiável：', len(r_data))\n",
    "\n",
    "train_data = pd.concat([un_data, r_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='novas_features'></a>\n",
    "## 1.8 Novos Recursos (Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, após os tratamentos dos dados, identificou-se a necessidade de criar novos recursos (feateares).\n",
    "A Análise de texto não é uma das tarefas mais fácil a se fazer, embora, seja possível por meio de extração de informações por palavras obter uma melhor compreensão da construção de um texto. O objetivo da análise foi identificar o volume de vezes que determinado texto contém: exclamação, questão, simbolo, palavras únicas e suas classes gramaticais como: substantivos, adjetivos e verbos.\n",
    "Entendemos ser importante identificar as entidades dentro dos contextos como: Pessoas, Grupos Politicos, Organizações, Valores monetários, Nações. Para extrair essas informações utilizou-se a biblioteca Spacy.\n",
    "Com esses novos recursos serão utilizados nas análises gráficas e no aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recursos_palavras'></a>\n",
    "### 1.8.1 Recursos por Palavras (Tokens) e Entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Função retorna a POS TAG')\n",
    "def tag_part_of_speech(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    pos_list = pos_tag(text_splited)\n",
    "    noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
    "    adjective_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
    "    verb_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
    "    return[noun_count, adjective_count, verb_count]\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Função retorna as Entidades')\n",
    "def entity(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    money = []\n",
    "    person = []\n",
    "    org = []\n",
    "    norp = []\n",
    "    gpe = []\n",
    "    fac = []\n",
    "    \n",
    "    conteudo = nlp(text)\n",
    "    \n",
    "    if conteudo.ents:\n",
    "        for ent in conteudo.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                person.append(ent.text)\n",
    "            elif ent.label_ == 'MONEY':\n",
    "                money.append(ent.text)\n",
    "            elif ent.label_ == 'ORG':\n",
    "                org.append(ent.text)\n",
    "            elif ent.label_ == 'NORP':\n",
    "                norp.append(ent.text)\n",
    "            elif ent.label_ == 'GPE':\n",
    "                gpe.append(ent.text)\n",
    "            elif ent.label_ == 'FAC':\n",
    "                fac.append(ent.text)\n",
    "                \n",
    "    return [str(person), str(money), str(org), str(norp), str(gpe), str(fac)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Geração de recursos baseados nos titulos\")\n",
    "for df in ([train_data]):\n",
    "    df['title_token'] = df['title'].apply(lambda x : tokenizer.tokenize(x.lower())) #[tokenizer.tokenize(x.lower()) for x in train_data['title']]\n",
    "    df['title_comprimento'] = df['title'].apply(lambda x : len(str(x)))\n",
    "    df['title_num_exclamação'] = df['title'].apply(lambda comment: str(comment).count('!'))\n",
    "    df['title_num_questao'] = df['title'].apply(lambda comment: str(comment).count('?')) \n",
    "    df['title_num_puntuacao'] = df['title'].apply(lambda comment: sum(str(comment).count(w) for w in '.,;:'))                                           \n",
    "    df['title_num_simbolo'] = df['title'].apply(lambda comment: sum(str(comment).count(w) for w in '*&$%'))                                           \n",
    "    df['title_num_palavras'] = df['title'].apply(lambda comment: len(str(comment).split()))                                         \n",
    "    df['title_num_palavras_unicas'] = df['title'].apply(lambda comment: len(set(w for w in str(comment).split())))                                      \n",
    "    df['title_palavras_vs_unico'] = df['title_num_palavras_unicas'] / df['title_num_palavras']\n",
    "    df['title_substantivos'], df['title_adjetivos'], df['title_verbos'] = zip(*df['title'].apply(lambda comment: tag_part_of_speech(str(comment))))\n",
    "    df['title_substantivos_vs_comprimento'] = df['title_substantivos'] / df['title_comprimento']\n",
    "    df['title_adjetivos_vs_comprimento'] = df['title_adjetivos'] / df['title_comprimento']\n",
    "    df['title_verbos_vs_comprimento'] = df['title_verbos'] /df['title_comprimento']\n",
    "    df['title_substantivos_vs_palavras'] = df['title_substantivos'] / df['title_num_palavras']\n",
    "    df['title_adjetivos_vs_palavras'] = df['title_adjetivos'] / df['title_num_palavras']\n",
    "    df['title_verbos_vs_palavras'] = df['title_verbos'] / df['title_num_palavras']\n",
    "    df['title_contagem_palavras'] = df['title'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    df['title_media_palavras_len'] = df['title'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    df['title_por_cento']= df['title_num_puntuacao']*100/df['title_num_palavras']\n",
    "    df['title_ent_person'], df['title_ent_money'], df['title_ent_org'], df['title_ent_norp'], df['title_ent_gpe'], df['title_ent_fac'] = zip(*df['title'].apply(lambda x: entity(str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Geração de recursos baseados nos textos\")\n",
    "for df in ([train_data]):\n",
    "    df['text_token'] = df['text'].apply(lambda x : tokenizer.tokenize(x.lower())) #[tokenizer.tokenize(x.lower()) for x in train_data['text']]\n",
    "    df['text_comprimento'] = df['text'].apply(lambda x : len(str(x)))\n",
    "    df['text_num_exclamação'] = df['text'].apply(lambda comment: str(comment).count('!'))\n",
    "    df['text_num_questao'] = df['text'].apply(lambda comment: str(comment).count('?')) \n",
    "    df['text_num_puntuacao'] = df['text'].apply(lambda comment: sum(str(comment).count(w) for w in '.,;:'))                                           \n",
    "    df['text_num_simbolo'] = df['text'].apply(lambda comment: sum(str(comment).count(w) for w in '*&$%'))                                           \n",
    "    df['text_num_palavras'] = df['text'].apply(lambda comment: len(str(comment).split()))                                         \n",
    "    df['text_num_palavras_unicas'] = df['text'].apply(lambda comment: len(set(w for w in str(comment).split())))                                      \n",
    "    df['text_palavras_vs_unico'] = df['text_num_palavras_unicas'] / df['text_num_palavras']\n",
    "    df['text_substantivos'], df['text_adjetivos'], df['text_verbos'] = zip(*df['text'].apply(lambda comment: tag_part_of_speech(str(comment))))\n",
    "    df['text_substantivos_vs_comprimento'] = df['text_substantivos'] / df['text_comprimento']\n",
    "    df['text_adjetivos_vs_comprimento'] = df['text_adjetivos'] / df['text_comprimento']\n",
    "    df['text_verbos_vs_comprimento'] = df['text_verbos'] /df['text_comprimento']\n",
    "    df['text_substantivos_vs_palavras'] = df['text_substantivos'] / df['text_num_palavras']\n",
    "    df['text_adjetivos_vs_palavras'] = df['text_adjetivos'] / df['text_num_palavras']\n",
    "    df['text_verbos_vs_palavras'] = df['text_verbos'] / df['text_num_palavras']\n",
    "    df['text_contagem_palavras'] = df['text'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    df['text_media_palavras_len'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    df['text_por_cento'] = df['text_num_puntuacao']*100/df['text_num_palavras']\n",
    "    df['text_ent_person'], df['text_ent_money'], df['text_ent_org'], df['text_ent_norp'], df['text_ent_gpe'], df['text_ent_fac'] = zip(*df['text'].apply(lambda x: entity(str(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='novas_features'></a>\n",
    "## 1.9 Análise Gráfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Gerar um WordCloud com todos os títulos e textos e analisar as palavras mais utilizadas.</li>\n",
    "    <li>Gerar um histograma referente ao tamanho dos títulos e textos. Será que existe diferença de tamanho (caracteres) para os títulos e textos confiáveis e não confiáveis?</li>\n",
    "    <li>Gerar um boxplot referente a quantidade de palavras dos títulos e textos e analisar os valores mínimos, máximos, primeiro e terceiro quartil, mediana e existência de outliers. Será que existe diferença de entre confiáveis e não confiáveis?</li>\n",
    "    <li>Podemos verificar alguma correlação entre os novos recursos?</li>\n",
    "    <li>A forma de escrita do texto (exemplos: educado, rude, gírias, etc…) tem influência no sentimento Confiável e Não Confiável?</li>    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando os textos confiáveis e não confiáveis para realizar as análises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unreliable = train_data[train_data['label'] == 1]\n",
    "reliable = train_data[train_data['label'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wordcloud'></a>\n",
    "### 1.9.1 WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carregando a lista de stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "## words referente a feature title\n",
    "words = []\n",
    "for p in train_data['title_token']:\n",
    "    if p not in stopwords:\n",
    "        words.append(p)\n",
    "\n",
    "words = str(words)\n",
    "            \n",
    "## parametro do wordcloud\n",
    "wordcloud = WordCloud(width=1000, height=800, margin=0)\n",
    "wordcloud.generate(words)\n",
    "    \n",
    "## plotagem\n",
    "plt.figure(figsize=(20,11))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.margins(x=0,y=0)\n",
    "plt.title('Wordcloud das Palavras dos Títulos')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carregando a lista de stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "## words referente a feature title\n",
    "words = []\n",
    "for p in train_data['text_token']:\n",
    "    if p not in stopwords:\n",
    "        words.append(p)\n",
    "\n",
    "words = str(words)\n",
    "            \n",
    "## parametro do wordcloud\n",
    "wordcloud = WordCloud(width=1000, height=800, margin=0)\n",
    "wordcloud.generate(words)\n",
    "    \n",
    "## plotagem\n",
    "plt.figure(figsize=(20,11))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.margins(x=0,y=0)\n",
    "plt.title('Wordcloud das Palavras dos Textos')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='histograma'></a>\n",
    "### 1.9.2 Histograma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A construção de histogramas tem caráter preliminar em qualquer estudo e é um importante indicador da distribuição de dados. Neste estudo utiliza-se a frequência absoluta, que é o número que representa a quantidade de dados em uma determinada amostra ou o intervalo de classe especifico, indicando a frequência (absoluta) com que uma classe aparece no conjunto de dados.</p>\n",
    "<p>A seguir, os gráficos abaixo representam os histogramas de Títulos e Textos. Estes gráficos facilitam no entendimento e na analise da existência de influência na avaliação: Confiável ou Não Confiável.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='histo_titulos'></a>\n",
    "#### 1.9.2.1 Histograma dos Títulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Não Confiáveis','Confiáveis']\n",
    "colors = ['#fb7082','#80b1d3']\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist([unreliable.title_comprimento,reliable.title_comprimento], bins=int(180/5), normed=False, color = colors, label=labels)\n",
    "plt.xlabel('Quantidade de Caracteres')\n",
    "plt.ylabel('Quantidade de Títulos')\n",
    "plt.title('Histograma do Total de Palavras dos Títulos')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='histo_textos'></a>\n",
    "#### 1.9.2.2 Histograma dos Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Não Confiáveis','Confiáveis']\n",
    "colors = ['#fb7082','#80b1d3']\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist([unreliable.text_comprimento,reliable.text_comprimento], bins=int(180/5), normed=False, color = colors, label=labels)\n",
    "plt.xlabel('Quantidade de Caracteres')\n",
    "plt.ylabel('Quantidade de Textos')\n",
    "plt.title('Histograma do Total de Palavras dos Textos')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boxplot'></a>\n",
    "### 1.9.3 Diagrama de Caixa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O diagrama de caixa é construído utilizando as referências de valores mínimos e máximos, primeiro e terceiro quartil, mediana e os outliers da base de dados. Diferentemente do histograma que é melhor para visualização de média e desvio padrão, o diagrama de caixa têm como objetivo estudar as medidas estatística e identificar os valores atípicos dentro do conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boxplot_titulo'></a>\n",
    "#### 1.9.3.1 Diagrama de Caixa dos Títulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confiavel = go.Box(y=reliable.title_comprimento, name = 'confiável', boxmean=True)\n",
    "nao_confiavel = go.Box(y=unreliable.title_comprimento, name = 'não confiáveis', boxmean=True)\n",
    "data = [confiavel, nao_confiavel]\n",
    "layout = go.Layout(title = \"Diagrama de Caixa do Total de Palavras dos Títulos\")\n",
    "fig = go.Figure(data=data,layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boxplot_texto'></a>\n",
    "#### 1.9.3.2 Diagrama de Caixa dos Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confiavel = go.Box(y=reliable.text_comprimento, name = 'confiável', boxmean=True)\n",
    "nao_confiavel = go.Box(y=unreliable.text_comprimento, name = 'não confiáveis', boxmean=True)\n",
    "data = [confiavel, nao_confiavel]\n",
    "layout = go.Layout(title = \"Diagrama de Caixa do Total de Palavras dos Textos\")\n",
    "fig = go.Figure(data=data,layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='matriz_correlacao'></a>\n",
    "### 1.9.5 Matriz de correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.init_notebook_mode(connected=True)\n",
    "pd.options.mode.chained_assignment = None\n",
    "np.random.seed(13)\n",
    "\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com as variáveis extraídas por meio da análise por palavra, se tornou possível criar relacionamentos e identicar quais desses relacionamento se correlacionam entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize= [20,15])\n",
    "sns.heatmap(reliable.drop(['id','title'], axis=1).corr(), annot=True, fmt=\".2f\", ax=ax, \n",
    "            cbar_kws={'label': 'Coeficiente de correlação'}, cmap='OrRd')\n",
    "ax.set_title(\"Matriz de Correlação da Análise por Palavra dos Textos Confiáveis\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize= [20,15])\n",
    "sns.heatmap(unreliable.drop(['id','title'], axis=1).corr(), annot=True, fmt=\".2f\", ax=ax, \n",
    "            cbar_kws={'label': 'Coeficiente de correlação'}, cmap='OrRd')\n",
    "ax.set_title(\"Matriz de Correlação da Análise por Palavra dos Textos Não Confiáveis\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='export_csv'></a>\n",
    "# 2. Exportando Dataframe Refatorado CSV:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sáida de todos os tratamentos dos dados será utilizada na próxima etapa que será o desenvolvimento de um aprendizado de máquina com deep leraning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv ('train_refatorado.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/shaz13/feature-engineering-for-nlp-classification/notebook#Tagging-Parts-Of-Speech-And-More-Feature-Engineering.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
